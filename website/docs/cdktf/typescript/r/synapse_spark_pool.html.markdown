---
subcategory: "Synapse"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_synapse_spark_pool"
description: |-
  Manages a Synapse Spark Pool.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_synapse_spark_pool

Manages a Synapse Spark Pool.

## Example Usage

```typescript
import * as constructs from "constructs";
import * as cdktf from "cdktf";
/*Provider bindings are generated by running cdktf get.
See https://cdk.tf/provider-generation for more details.*/
import * as azurerm from "./.gen/providers/azurerm";
class MyConvertedCode extends cdktf.TerraformStack {
  constructor(scope: constructs.Construct, name: string) {
    super(scope, name);
    const azurermResourceGroupExample = new azurerm.resourceGroup.ResourceGroup(
      this,
      "example",
      {
        location: "West Europe",
        name: "example-resources",
      }
    );
    const azurermStorageAccountExample =
      new azurerm.storageAccount.StorageAccount(this, "example_1", {
        accountKind: "StorageV2",
        accountReplicationType: "LRS",
        accountTier: "Standard",
        isHnsEnabled: cdktf.Token.asBoolean("true"),
        location: cdktf.Token.asString(azurermResourceGroupExample.location),
        name: "examplestorageacc",
        resourceGroupName: cdktf.Token.asString(
          azurermResourceGroupExample.name
        ),
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermStorageAccountExample.overrideLogicalId("example");
    const azurermStorageDataLakeGen2FilesystemExample =
      new azurerm.storageDataLakeGen2Filesystem.StorageDataLakeGen2Filesystem(
        this,
        "example_2",
        {
          name: "example",
          storageAccountId: cdktf.Token.asString(
            azurermStorageAccountExample.id
          ),
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermStorageDataLakeGen2FilesystemExample.overrideLogicalId("example");
    const azurermSynapseWorkspaceExample =
      new azurerm.synapseWorkspace.SynapseWorkspace(this, "example_3", {
        identity: {
          type: "SystemAssigned",
        },
        location: cdktf.Token.asString(azurermResourceGroupExample.location),
        name: "example",
        resourceGroupName: cdktf.Token.asString(
          azurermResourceGroupExample.name
        ),
        sqlAdministratorLogin: "sqladminuser",
        sqlAdministratorLoginPassword: "H@Sh1CoR3!",
        storageDataLakeGen2FilesystemId: cdktf.Token.asString(
          azurermStorageDataLakeGen2FilesystemExample.id
        ),
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermSynapseWorkspaceExample.overrideLogicalId("example");
    const azurermSynapseSparkPoolExample =
      new azurerm.synapseSparkPool.SynapseSparkPool(this, "example_4", {
        autoPause: {
          delayInMinutes: 15,
        },
        autoScale: {
          maxNodeCount: 50,
          minNodeCount: 3,
        },
        cacheSize: 100,
        libraryRequirement: {
          content: "appnope==0.1.0\nbeautifulsoup4==4.6.3\n",
          filename: "requirements.txt",
        },
        name: "example",
        nodeSize: "Small",
        nodeSizeFamily: "MemoryOptimized",
        sparkConfig: {
          content: "spark.shuffle.spill                true\n",
          filename: "config.txt",
        },
        synapseWorkspaceId: cdktf.Token.asString(
          azurermSynapseWorkspaceExample.id
        ),
        tags: {
          ENV: "Production",
        },
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermSynapseSparkPoolExample.overrideLogicalId("example");
  }
}

```

## Arguments Reference

The following arguments are supported:

* `name` - (Required) The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.

* `synapseWorkspaceId` - (Required) The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.

* `nodeSizeFamily` - (Required) The kind of nodes that the Spark Pool provides. Possible values are `memoryOptimized` and `none`.

* `nodeSize` - (Required) The level of node in the Spark Pool. Possible values are `small`, `medium`, `large`, `none`, `xLarge`, `xxLarge` and `xxxLarge`.

* `nodeCount` - (Optional) The number of nodes in the Spark Pool. Exactly one of `nodeCount` or `autoScale` must be specified.

* `autoScale` - (Optional) An `autoScale` block as defined below. Exactly one of `nodeCount` or `autoScale` must be specified.

* `autoPause` - (Optional) An `autoPause` block as defined below.

* `cacheSize` - (Optional) The cache size in the Spark Pool.

* `computeIsolationEnabled` - (Optional) Indicates whether compute isolation is enabled or not. Defaults to `false`.

~> **NOTE:** The `computeIsolationEnabled` is only available with the XXXLarge (80 vCPU / 504 GB) node size and only available in the following regions: East US, West US 2, South Central US, US Gov Arizona, US Gov Virginia. See [Isolated Compute](https://docs.microsoft.com/azure/synapse-analytics/spark/apache-spark-pool-configurations#isolated-compute) for more information.

* `dynamicExecutorAllocationEnabled` - (Optional) Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.

* `minExecutors` - (Optional) The minimum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.

* `maxExecutors` - (Optional) The maximum number of executors allocated only when `dynamicExecutorAllocationEnabled` set to `true`.
  
* `libraryRequirement` - (Optional) A `libraryRequirement` block as defined below.

* `sessionLevelPackagesEnabled` - (Optional) Indicates whether session level packages are enabled or not. Defaults to `false`.

* `sparkConfig` - (Optional) A `sparkConfig` block as defined below.

* `sparkLogFolder` - (Optional) The default folder where Spark logs will be written. Defaults to `/logs`.

* `sparkEventsFolder` - (Optional) The Spark events folder. Defaults to `/events`.

* `sparkVersion` - (Optional) The Apache Spark version. Possible values are `24` , `31` , `32` and `33`. Defaults to `24`.

* `tags` - (Optional) A mapping of tags which should be assigned to the Synapse Spark Pool.

---

An `autoPause` block supports the following:

* `delayInMinutes` - (Required) Number of minutes of idle time before the Spark Pool is automatically paused. Must be between `5` and `10080`.

---

An `autoScale` block supports the following:

* `maxNodeCount` - (Required) The maximum number of nodes the Spark Pool can support. Must be between `3` and `200`.

* `minNodeCount` - (Required) The minimum number of nodes the Spark Pool can support. Must be between `3` and `200`.

---

An `libraryRequirement` block supports the following:

* `content` - (Required) The content of library requirements.

* `filename` - (Required) The name of the library requirements file.

---

An `sparkConfig` block supports the following:

* `content` - (Required) The contents of a spark configuration.

* `filename` - (Required) The name of the file where the spark configuration `content` will be stored.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Synapse Spark Pool.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/language/resources/syntax#operation-timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Synapse Spark Pool.
* `read` - (Defaults to 5 minutes) Used when retrieving the Synapse Spark Pool.
* `update` - (Defaults to 30 minutes) Used when updating the Synapse Spark Pool.
* `delete` - (Defaults to 30 minutes) Used when deleting the Synapse Spark Pool.

## Import

Synapse Spark Pool can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_synapse_spark_pool.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-34a4ca362bab762e262355044fe626a95f2ccd30a1f3762fbb4cad31fdc67558 -->