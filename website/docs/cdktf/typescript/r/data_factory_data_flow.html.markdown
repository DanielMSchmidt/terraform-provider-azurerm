---
subcategory: "Data Factory"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_data_factory_data_flow"
description: |-
  Manages a Data Flow inside an Azure Data Factory.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_data_factory_data_flow

Manages a Data Flow inside an Azure Data Factory.

## Example Usage

```typescript
import * as constructs from "constructs";
import * as cdktf from "cdktf";
/*Provider bindings are generated by running cdktf get.
See https://cdk.tf/provider-generation for more details.*/
import * as azurerm from "./.gen/providers/azurerm";
class MyConvertedCode extends cdktf.TerraformStack {
  constructor(scope: constructs.Construct, name: string) {
    super(scope, name);
    const azurermResourceGroupExample = new azurerm.resourceGroup.ResourceGroup(
      this,
      "example",
      {
        location: "West Europe",
        name: "example-resources",
      }
    );
    const azurermStorageAccountExample =
      new azurerm.storageAccount.StorageAccount(this, "example_1", {
        accountReplicationType: "LRS",
        accountTier: "Standard",
        location: cdktf.Token.asString(azurermResourceGroupExample.location),
        name: "example",
        resourceGroupName: cdktf.Token.asString(
          azurermResourceGroupExample.name
        ),
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermStorageAccountExample.overrideLogicalId("example");
    const azurermDataFactoryExample = new azurerm.dataFactory.DataFactory(
      this,
      "example_2",
      {
        location: cdktf.Token.asString(azurermResourceGroupExample.location),
        name: "example",
        resourceGroupName: cdktf.Token.asString(
          azurermResourceGroupExample.name
        ),
      }
    );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryExample.overrideLogicalId("example");
    const azurermDataFactoryLinkedCustomServiceExample =
      new azurerm.dataFactoryLinkedCustomService.DataFactoryLinkedCustomService(
        this,
        "example_3",
        {
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          name: "linked_service",
          type: "AzureBlobStorage",
          typePropertiesJson:
            '{\n  "connectionString": "${' +
            azurermStorageAccountExample.primaryConnectionString +
            '}"\n}\n',
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryLinkedCustomServiceExample.overrideLogicalId("example");
    const azurermDataFactoryDatasetJsonExample1 =
      new azurerm.dataFactoryDatasetJson.DataFactoryDatasetJson(
        this,
        "example1",
        {
          azureBlobStorageLocation: {
            container: "container",
            filename: "foo.txt",
            path: "foo/bar/",
          },
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          encoding: "UTF-8",
          linkedServiceName: cdktf.Token.asString(
            azurermDataFactoryLinkedCustomServiceExample.name
          ),
          name: "dataset1",
        }
      );
    const azurermDataFactoryDatasetJsonExample2 =
      new azurerm.dataFactoryDatasetJson.DataFactoryDatasetJson(
        this,
        "example2",
        {
          azureBlobStorageLocation: {
            container: "container",
            filename: "bar.txt",
            path: "foo/bar/",
          },
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          encoding: "UTF-8",
          linkedServiceName: cdktf.Token.asString(
            azurermDataFactoryLinkedCustomServiceExample.name
          ),
          name: "dataset2",
        }
      );
    const azurermDataFactoryFlowletDataFlowExample1 =
      new azurerm.dataFactoryFlowletDataFlow.DataFactoryFlowletDataFlow(
        this,
        "example1_6",
        {
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          name: "example",
          script:
            "source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
          sink: [
            {
              linkedService: {
                name: cdktf.Token.asString(
                  azurermDataFactoryLinkedCustomServiceExample.name
                ),
              },
              name: "sink1",
            },
          ],
          source: [
            {
              linkedService: {
                name: cdktf.Token.asString(
                  azurermDataFactoryLinkedCustomServiceExample.name
                ),
              },
              name: "source1",
            },
          ],
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryFlowletDataFlowExample1.overrideLogicalId("example1");
    const azurermDataFactoryFlowletDataFlowExample2 =
      new azurerm.dataFactoryFlowletDataFlow.DataFactoryFlowletDataFlow(
        this,
        "example2_7",
        {
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          name: "example",
          script:
            "source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
          sink: [
            {
              linkedService: {
                name: cdktf.Token.asString(
                  azurermDataFactoryLinkedCustomServiceExample.name
                ),
              },
              name: "sink1",
            },
          ],
          source: [
            {
              linkedService: {
                name: cdktf.Token.asString(
                  azurermDataFactoryLinkedCustomServiceExample.name
                ),
              },
              name: "source1",
            },
          ],
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryFlowletDataFlowExample2.overrideLogicalId("example2");
    const azurermDataFactoryDataFlowExample =
      new azurerm.dataFactoryDataFlow.DataFactoryDataFlow(this, "example_8", {
        dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
        name: "example",
        script:
          "source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
        sink: [
          {
            dataset: {
              name: cdktf.Token.asString(
                azurermDataFactoryDatasetJsonExample2.name
              ),
            },
            flowlet: {
              name: cdktf.Token.asString(
                azurermDataFactoryFlowletDataFlowExample2.name
              ),
              parameters: {
                Key1: "value1",
              },
            },
            name: "sink1",
          },
        ],
        source: [
          {
            dataset: {
              name: cdktf.Token.asString(
                azurermDataFactoryDatasetJsonExample1.name
              ),
            },
            flowlet: {
              name: cdktf.Token.asString(
                azurermDataFactoryFlowletDataFlowExample1.name
              ),
              parameters: {
                Key1: "value1",
              },
            },
            name: "source1",
          },
        ],
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryDataFlowExample.overrideLogicalId("example");
  }
}

```

## Argument Reference

The following arguments are supported:

* `name` - (Required) Specifies the name of the Data Factory Data Flow. Changing this forces a new resource to be created.

* `dataFactoryId` - (Required) The ID of Data Factory in which to associate the Data Flow with. Changing this forces a new resource.

* `script` - (Optional) The script for the Data Factory Data Flow.

* `scriptLines` - (Optional) The script lines for the Data Factory Data Flow.

* `source` - (Required) One or more `source` blocks as defined below.

* `sink` - (Required) One or more `sink` blocks as defined below.

* `annotations` - (Optional) List of tags that can be used for describing the Data Factory Data Flow.

* `description` - (Optional) The description for the Data Factory Data Flow.

* `folder` - (Optional) The folder that this Data Flow is in. If not specified, the Data Flow will appear at the root level.

* `transformation` - (Optional) One or more `transformation` blocks as defined below.

---

A `source` block supports the following:

* `name` - (Required) The name for the Data Flow Source.

* `description` - (Optional) The description for the Data Flow Source.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linkedService` - (Optional) A `linkedService` block as defined below.

* `rejectedLinkedService` - (Optional) A `rejectedLinkedService` block as defined below.

* `schemaLinkedService` - (Optional) A `schemaLinkedService` block as defined below.

---

A `sink` block supports the following:

* `name` - (Required) The name for the Data Flow Source.

* `description` - (Optional) The description for the Data Flow Source.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linkedService` - (Optional) A `linkedService` block as defined below.

* `rejectedLinkedService` - (Optional) A `rejectedLinkedService` block as defined below.

* `schemaLinkedService` - (Optional) A `schemaLinkedService` block as defined below.

---

A `dataset` block supports the following:

* `name` - (Required) The name for the Data Factory Dataset.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory dataset.

---

A `flowlet` block supports the following:

* `name` - (Required) The name for the Data Factory Flowlet.

* `datasetParameters` - (Optional) Specifies the reference data flow parameters from dataset.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Flowlet.

---

A `linkedService` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `rejectedLinkedService` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service with schema.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `schemaLinkedService` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service with schema.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `transformation` block supports the following:

* `name` - (Required) The name for the Data Flow transformation.

* `description` - (Optional) The description for the Data Flow transformation.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linkedService` - (Optional) A `linkedService` block as defined below.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Data Factory Data Flow.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/language/resources/syntax#operation-timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Data Factory Data Flow.
* `update` - (Defaults to 30 minutes) Used when updating the Data Factory Data Flow.
* `read` - (Defaults to 5 minutes) Used when retrieving the Data Factory Data Flow.
* `delete` - (Defaults to 30 minutes) Used when deleting the Data Factory Data Flow.

## Import

Data Factory Data Flow can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_data_factory_data_flow.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/dataflows/example
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-6b310ab72877aaab1e80449cc255281afde6b1acefd62d73b149bc1168d53f4f -->