---
subcategory: "Data Factory"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_data_factory_dataset_parquet"
description: |-
  Manages an Azure Parquet Dataset inside an Azure Data Factory.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_data_factory_dataset_parquet

Manages an Azure Parquet Dataset inside an Azure Data Factory.

## Example Usage

```typescript
import * as constructs from "constructs";
import * as cdktf from "cdktf";
/*Provider bindings are generated by running cdktf get.
See https://cdk.tf/provider-generation for more details.*/
import * as azurerm from "./.gen/providers/azurerm";
class MyConvertedCode extends cdktf.TerraformStack {
  constructor(scope: constructs.Construct, name: string) {
    super(scope, name);
    const azurermResourceGroupExample = new azurerm.resourceGroup.ResourceGroup(
      this,
      "example",
      {
        location: "West Europe",
        name: "example-resources",
      }
    );
    const azurermDataFactoryExample = new azurerm.dataFactory.DataFactory(
      this,
      "example_1",
      {
        location: cdktf.Token.asString(azurermResourceGroupExample.location),
        name: "example",
        resourceGroupName: cdktf.Token.asString(
          azurermResourceGroupExample.name
        ),
      }
    );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryExample.overrideLogicalId("example");
    const azurermDataFactoryLinkedServiceWebExample =
      new azurerm.dataFactoryLinkedServiceWeb.DataFactoryLinkedServiceWeb(
        this,
        "example_2",
        {
          authenticationType: "Anonymous",
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          name: "example",
          url: "https://www.bing.com",
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryLinkedServiceWebExample.overrideLogicalId("example");
    const azurermDataFactoryDatasetParquetExample =
      new azurerm.dataFactoryDatasetParquet.DataFactoryDatasetParquet(
        this,
        "example_3",
        {
          dataFactoryId: cdktf.Token.asString(azurermDataFactoryExample.id),
          httpServerLocation: {
            filename: "fizz.txt",
            path: "foo/bar/",
            relativeUrl: "http://www.bing.com",
          },
          linkedServiceName: cdktf.Token.asString(
            azurermDataFactoryLinkedServiceWebExample.name
          ),
          name: "example",
        }
      );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    azurermDataFactoryDatasetParquetExample.overrideLogicalId("example");
  }
}

```

## Argument Reference

The following supported arguments are common across all Azure Data Factory Datasets:

* `name` - (Required) Specifies the name of the Data Factory Dataset. Changing this forces a new resource to be created. Must be globally unique. See the [Microsoft documentation](https://docs.microsoft.com/azure/data-factory/naming-rules) for all restrictions.

* `dataFactoryId` - (Required) The Data Factory ID in which to associate the Dataset with. Changing this forces a new resource.

* `linkedServiceName` - (Required) The Data Factory Linked Service name in which to associate the Dataset with.

* `folder` - (Optional) The folder that this Dataset is in. If not specified, the Dataset will appear at the root level.

* `schemaColumn` - (Optional) A `schemaColumn` block as defined below.

* `description` - (Optional) The description for the Data Factory Dataset.

* `annotations` - (Optional) List of tags that can be used for describing the Data Factory Dataset.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Dataset.

* `additionalProperties` - (Optional) A map of additional properties to associate with the Data Factory Dataset.

The following supported locations for a Parquet Dataset:

* `httpServerLocation` - (Optional) A `httpServerLocation` block as defined below.

* `azureBlobStorageLocation` - (Optional) A `azureBlobStorageLocation` block as defined below.

The following supported arguments are specific to Parquet Dataset:

* `compressionCodec` - (Optional) The compression codec used to read/write text files. Valid values are `bzip2`, `gzip`, `deflate`, `zipDeflate`, `tarGzip`, `tar`, `snappy`, or `lz4`. Please note these values are case-sensitive.

* `compressionLevel` - (Optional) Specifies the compression level. Possible values are `optimal` and `fastest`,

---

A `schemaColumn` block supports the following:

* `name` - (Required) The name of the column.

* `type` - (Optional) Type of the column. Valid values are `byte`, `byte[]`, `boolean`, `date`, `dateTime`,`dateTimeOffset`, `decimal`, `double`, `guid`, `int16`, `int32`, `int64`, `single`, `string`, `timeSpan`. Please note these values are case sensitive.

* `description` - (Optional) The description of the column.

---

A `httpServerLocation` block supports the following:

* `relativeUrl` - (Required) The base URL to the web server hosting the file.

* `filename` - (Required) The filename of the file on the web server.

* `dynamicPathEnabled` - (Optional) Is the `path` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamicFilenameEnabled` - (Optional) Is the `filename` using dynamic expression, function or system variables? Defaults to `false`.

* `path` - (Optional) The folder path to the file on the web server.
---

A `azureBlobStorageLocation` block supports the following:

* `container` - (Required) The container on the Azure Blob Storage Account hosting the file.

* `filename` - (Optional) The filename of the file on the web server.

* `dynamicContainerEnabled` - (Optional) Is the `container` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamicPathEnabled` - (Optional) Is the `path` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamicFilenameEnabled` - (Optional) Is the `filename` using dynamic expression, function or system variables? Defaults to `false`.

* `path` - (Optional) The folder path to the file on the web server.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Data Factory Dataset.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/language/resources/syntax#operation-timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Data Factory Dataset.
* `update` - (Defaults to 30 minutes) Used when updating the Data Factory Dataset.
* `read` - (Defaults to 5 minutes) Used when retrieving the Data Factory Dataset.
* `delete` - (Defaults to 30 minutes) Used when deleting the Data Factory Dataset.

## Import

Data Factory Datasets can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_data_factory_dataset_parquet.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/datasets/example
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-ac5f9fe2eb728a42a55415057350ee52c55f67ab5417503406a2f167767c72ea -->