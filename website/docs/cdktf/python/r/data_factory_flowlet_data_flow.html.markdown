---
subcategory: "Data Factory"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_data_factory_flowlet_data_flow"
description: |-
  Manages a Flowlet Data Flow inside an Azure Data Factory.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_data_factory_flowlet_data_flow

Manages a Flowlet Data Flow inside an Azure Data Factory.

## Example Usage

```python
import constructs as constructs
import cdktf as cdktf
# Provider bindings are generated by running cdktf get.
# See https://cdk.tf/provider-generation for more details.
import ...gen.providers.azurerm as azurerm
class MyConvertedCode(cdktf.TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        azurerm_resource_group_example = azurerm.resource_group.ResourceGroup(self, "example",
            location="West Europe",
            name="example-resources"
        )
        azurerm_storage_account_example =
        azurerm.storage_account.StorageAccount(self, "example_1",
            account_replication_type="LRS",
            account_tier="Standard",
            location=cdktf.Token.as_string(azurerm_resource_group_example.location),
            name="example",
            resource_group_name=cdktf.Token.as_string(azurerm_resource_group_example.name)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_storage_account_example.override_logical_id("example")
        azurerm_data_factory_example = azurerm.data_factory.DataFactory(self, "example_2",
            location=cdktf.Token.as_string(azurerm_resource_group_example.location),
            name="example",
            resource_group_name=cdktf.Token.as_string(azurerm_resource_group_example.name)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_example.override_logical_id("example")
        azurerm_data_factory_linked_custom_service_example =
        azurerm.data_factory_linked_custom_service.DataFactoryLinkedCustomService(self, "example_3",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            name="linked_service",
            type="AzureBlobStorage",
            type_properties_json="{\n  \"connectionString\": \"${" + azurerm_storage_account_example.primary_connection_string + "}\"\n}\n"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_linked_custom_service_example.override_logical_id("example")
        azurerm.data_factory_dataset_json.DataFactoryDatasetJson(self, "example1",
            azure_blob_storage_location=DataFactoryDatasetJsonAzureBlobStorageLocation(
                container="container",
                filename="foo.txt",
                path="foo/bar/"
            ),
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            encoding="UTF-8",
            linked_service_name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name),
            name="dataset1"
        )
        azurerm.data_factory_dataset_json.DataFactoryDatasetJson(self, "example2",
            azure_blob_storage_location=DataFactoryDatasetJsonAzureBlobStorageLocation(
                container="container",
                filename="bar.txt",
                path="foo/bar/"
            ),
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            encoding="UTF-8",
            linked_service_name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name),
            name="dataset2"
        )
        azurerm_data_factory_flowlet_data_flow_example1 =
        azurerm.data_factory_flowlet_data_flow.DataFactoryFlowletDataFlow(self, "example1_6",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            name="example",
            script="source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
            sink=[DataFactoryFlowletDataFlowSink(
                linked_service=DataFactoryFlowletDataFlowSinkLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="sink1"
            )
            ],
            source=[DataFactoryFlowletDataFlowSource(
                linked_service=DataFactoryFlowletDataFlowSourceLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="source1"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_flowlet_data_flow_example1.override_logical_id("example1")
        azurerm_data_factory_flowlet_data_flow_example2 =
        azurerm.data_factory_flowlet_data_flow.DataFactoryFlowletDataFlow(self, "example2_7",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            name="example",
            script="source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
            sink=[DataFactoryFlowletDataFlowSink(
                linked_service=DataFactoryFlowletDataFlowSinkLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="sink1"
            )
            ],
            source=[DataFactoryFlowletDataFlowSource(
                linked_service=DataFactoryFlowletDataFlowSourceLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="source1"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_flowlet_data_flow_example2.override_logical_id("example2")
        azurerm_data_factory_flowlet_data_flow_example =
        azurerm.data_factory_flowlet_data_flow.DataFactoryFlowletDataFlow(self, "example_8",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            name="example",
            script="source(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  limit: 100, \n  ignoreNoFilesFound: false, \n  documentForm: 'documentPerLine') ~> source1 \nsource1 sink(\n  allowSchemaDrift: true, \n  validateSchema: false, \n  skipDuplicateMapInputs: true, \n  skipDuplicateMapOutputs: true) ~> sink1\n",
            sink=[DataFactoryFlowletDataFlowSink(
                flowlet=DataFactoryFlowletDataFlowSinkFlowlet(
                    name=cdktf.Token.as_string(azurerm_data_factory_flowlet_data_flow_example2.name)
                ),
                linked_service=DataFactoryFlowletDataFlowSinkLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="sink1"
            )
            ],
            source=[DataFactoryFlowletDataFlowSource(
                flowlet=DataFactoryFlowletDataFlowSourceFlowlet(
                    name=cdktf.Token.as_string(azurerm_data_factory_flowlet_data_flow_example1.name)
                ),
                linked_service=DataFactoryFlowletDataFlowSourceLinkedService(
                    name=cdktf.Token.as_string(azurerm_data_factory_linked_custom_service_example.name)
                ),
                name="source1"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_flowlet_data_flow_example.override_logical_id("example")
```

## Argument Reference

The following arguments are supported:

* `annotations` - (Optional) List of tags that can be used for describing the Data Factory Flowlet Data Flow.

* `data_factory_id` - (Required) The ID of Data Factory in which to associate the Data Flow with. Changing this forces a new resource.

* `name` - (Required) Specifies the name of the Data Factory Flowlet Data Flow. Changing this forces a new resource to be created.

* `description` - (Optional) The description for the Data Factory Flowlet Data Flow.

* `folder` - (Optional) The folder that this Data Flow is in. If not specified, the Data Flow will appear at the root level.

* `source` - (Required) One or more `source` blocks as defined below.

* `sink` - (Required) One or more `sink` blocks as defined below.

* `script` - (Optional) The script for the Data Factory Flowlet Data Flow.

* `script_lines` - (Optional) The script lines for the Data Factory Flowlet Data Flow.

* `transformation` - (Optional) One or more `transformation` blocks as defined below.

---

A `dataset` block supports the following:

* `name` - (Required) The name for the Data Factory Dataset.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory dataset.

---

A `flowlet` block supports the following:

* `name` - (Required) The name for the Data Factory Flowlet.

* `dataset_parameters` - (Optional) Specifies the reference data flow parameters from dataset.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Flowlet.

---

A `linked_service` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `source` block supports the following:

* `description` - (Optional) The description for the Data Flow Source.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linked_service` - (Optional) A `linked_service` block as defined below.

* `name` - (Required) The name for the Data Flow Source.

* `rejected_linked_service` - (Optional) A `rejected_linked_service` block as defined below.

* `schema_linked_service` - (Optional) A `schema_linked_service` block as defined below.

---

A `sink` block supports the following:

* `description` - (Optional) The description for the Data Flow Source.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linked_service` - (Optional) A `linked_service` block as defined below.

* `name` - (Required) The name for the Data Flow Source.

* `rejected_linked_service` - (Optional) A `rejected_linked_service` block as defined below.

* `schema_linked_service` - (Optional) A `schema_linked_service` block as defined below.

---

A `rejected_linked_service` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service with schema.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `schema_linked_service` block supports the following:

* `name` - (Required) The name for the Data Factory Linked Service with schema.

* `parameters` - (Optional) A map of parameters to associate with the Data Factory Linked Service.

---

A `transformation` block supports the following:

* `name` - (Required) The name for the Data Flow transformation.

* `description` - (Optional) The description for the Data Flow transformation.

* `dataset` - (Optional) A `dataset` block as defined below.

* `flowlet` - (Optional) A `flowlet` block as defined below.

* `linked_service` - (Optional) A `linked_service` block as defined below.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Data Factory Flowlet Data Flow.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/docs/configuration/resources.html#timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Data Factory Flowlet Data Flow.
* `update` - (Defaults to 30 minutes) Used when updating the Data Factory Flowlet Data Flow.
* `read` - (Defaults to 5 minutes) Used when retrieving the Data Factory Flowlet Data Flow.
* `delete` - (Defaults to 30 minutes) Used when deleting the Data Factory Flowlet Data Flow.

## Import

Data Factory Flowlet Data Flow can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_data_factory_flowlet_data_flow.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/dataflows/example
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-452b7f2c61aaa52ae6ee639a2211a402f01806566f86423e0985a5d497f361c8 -->