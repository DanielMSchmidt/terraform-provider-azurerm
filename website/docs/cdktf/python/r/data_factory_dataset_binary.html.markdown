---
subcategory: "Data Factory"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_data_factory_dataset_binary"
description: |-
  Manages a Data Factory Binary Dataset inside an Azure Data Factory.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_data_factory_dataset_binary

Manages a Data Factory Binary Dataset inside an Azure Data Factory.

## Example Usage

```python
import constructs as constructs
import cdktf as cdktf
# Provider bindings are generated by running cdktf get.
# See https://cdk.tf/provider-generation for more details.
import ...gen.providers.azurerm as azurerm
class MyConvertedCode(cdktf.TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        azurerm_resource_group_example = azurerm.resource_group.ResourceGroup(self, "example",
            location="West Europe",
            name="example"
        )
        azurerm_data_factory_example = azurerm.data_factory.DataFactory(self, "example_1",
            location=cdktf.Token.as_string(azurerm_resource_group_example.location),
            name="example",
            resource_group_name=cdktf.Token.as_string(azurerm_resource_group_example.name)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_example.override_logical_id("example")
        azurerm_data_factory_linked_service_sftp_example =
        azurerm.data_factory_linked_service_sftp.DataFactoryLinkedServiceSftp(self, "example_2",
            authentication_type="Basic",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            host="http://www.bing.com",
            name="example",
            password="bar",
            port=22,
            username="foo"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_linked_service_sftp_example.override_logical_id("example")
        azurerm_data_factory_dataset_binary_example =
        azurerm.data_factory_dataset_binary.DataFactoryDatasetBinary(self, "example_3",
            data_factory_id=cdktf.Token.as_string(azurerm_data_factory_example.id),
            linked_service_name=cdktf.Token.as_string(azurerm_data_factory_linked_service_sftp_example.name),
            name="example",
            sftp_server_location=DataFactoryDatasetBinarySftpServerLocation(
                filename="**",
                path="/test/"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_data_factory_dataset_binary_example.override_logical_id("example")
```

## Arguments Reference

The following arguments are supported:

* `name` - (Required) Specifies the name of the Data Factory Binary Dataset. Changing this forces a new resource to be created. Must be globally unique. See the [Microsoft documentation](https://docs.microsoft.com/azure/data-factory/naming-rules) for all restrictions.

* `data_factory_id` - (Required) The Data Factory ID in which to associate the Linked Service with. Changing this forces a new resource.

* `linked_service_name` - (Required) The Data Factory Linked Service name in which to associate the Binary Dataset with.

---

* `additional_properties` - (Optional) A map of additional properties to associate with the Data Factory Binary Dataset.

* `annotations` - (Optional) List of tags that can be used for describing the Data Factory Binary Dataset.

* `compression` - (Optional) A `compression` block as defined below.

* `description` - (Optional) The description for the Data Factory Dataset.

* `folder` - (Optional) The folder that this Dataset is in. If not specified, the Dataset will appear at the root level.

* `parameters` - (Optional) Specifies a list of parameters to associate with the Data Factory Binary Dataset.

The following supported locations for a Binary Dataset. One of these should be specified:

* `http_server_location` - (Optional) A `http_server_location` block as defined below.

* `azure_blob_storage_location` - (Optional) A `azure_blob_storage_location` block as defined below.

* `sftp_server_location` - (Optional) A `sftp_server_location` block as defined below.

---

A `compression` block supports the following:

* `type` - (Required) The type of compression used during transport. Possible values are `BZip2`, `Deflate`, `GZip`, `Tar`, `TarGZip` and `ZipDeflate`.

* `level` - (Optional) The level of compression. Possible values are `Fastest` and `Optimal`.

---

A `http_server_location` block supports the following:

* `relative_url` - (Required) The base URL to the web server hosting the file.

* `path` - (Required) The folder path to the file on the web server.

* `filename` - (Required) The filename of the file on the web server.

* `dynamic_path_enabled` - (Optional) Is the `path` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamic_filename_enabled` - (Optional) Is the `filename` using dynamic expression, function or system variables? Defaults to `false`.

---

A `azure_blob_storage_location` block supports the following:

* `container` - (Required) The container on the Azure Blob Storage Account hosting the file.

* `path` - (Optional) The folder path to the file in the blob container.

* `filename` - (Optional) The filename of the file in the blob container.

* `dynamic_container_enabled` - (Optional) Is the `container` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamic_path_enabled` - (Optional) Is the `path` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamic_filename_enabled` - (Optional) Is the `filename` using dynamic expression, function or system variables? Defaults to `false`.

---

A `sftp_server_location` block supports the following:

* `path` - (Required) The folder path to the file on the SFTP server.

* `filename` - (Required) The filename of the file on the SFTP server.

* `dynamic_path_enabled` - (Optional) Is the `path` using dynamic expression, function or system variables? Defaults to `false`.

* `dynamic_filename_enabled` - (Optional) Is the `filename` using dynamic expression, function or system variables? Defaults to `false`.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Data Factory Dataset.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/language/resources/syntax#operation-timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Data Factory Dataset.
* `read` - (Defaults to 5 minutes) Used when retrieving the Data Factory Dataset.
* `update` - (Defaults to 30 minutes) Used when updating the Data Factory Dataset.
* `delete` - (Defaults to 30 minutes) Used when deleting the Data Factory Dataset.

## Import

Data Factory Binary Datasets can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_data_factory_dataset_binary.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example/providers/Microsoft.DataFactory/factories/example/datasets/example
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-53e12ab0c82c529b5879563213b4a1e2177c740778b831182dc3b17014a69af1 -->