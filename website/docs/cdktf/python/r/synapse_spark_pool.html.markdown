---
subcategory: "Synapse"
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_synapse_spark_pool"
description: |-
  Manages a Synapse Spark Pool.
---


<!-- Please do not edit this file, it is generated. -->
# azurerm_synapse_spark_pool

Manages a Synapse Spark Pool.

## Example Usage

```python
import constructs as constructs
import cdktf as cdktf
# Provider bindings are generated by running cdktf get.
# See https://cdk.tf/provider-generation for more details.
import ...gen.providers.azurerm as azurerm
class MyConvertedCode(cdktf.TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        azurerm_resource_group_example = azurerm.resource_group.ResourceGroup(self, "example",
            location="West Europe",
            name="example-resources"
        )
        azurerm_storage_account_example =
        azurerm.storage_account.StorageAccount(self, "example_1",
            account_kind="StorageV2",
            account_replication_type="LRS",
            account_tier="Standard",
            is_hns_enabled=cdktf.Token.as_boolean("true"),
            location=cdktf.Token.as_string(azurerm_resource_group_example.location),
            name="examplestorageacc",
            resource_group_name=cdktf.Token.as_string(azurerm_resource_group_example.name)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_storage_account_example.override_logical_id("example")
        azurerm_storage_data_lake_gen2_filesystem_example =
        azurerm.storage_data_lake_gen2_filesystem.StorageDataLakeGen2Filesystem(self, "example_2",
            name="example",
            storage_account_id=cdktf.Token.as_string(azurerm_storage_account_example.id)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_storage_data_lake_gen2_filesystem_example.override_logical_id("example")
        azurerm_synapse_workspace_example =
        azurerm.synapse_workspace.SynapseWorkspace(self, "example_3",
            identity=SynapseWorkspaceIdentity(
                type="SystemAssigned"
            ),
            location=cdktf.Token.as_string(azurerm_resource_group_example.location),
            name="example",
            resource_group_name=cdktf.Token.as_string(azurerm_resource_group_example.name),
            sql_administrator_login="sqladminuser",
            sql_administrator_login_password="H@Sh1CoR3!",
            storage_data_lake_gen2_filesystem_id=cdktf.Token.as_string(azurerm_storage_data_lake_gen2_filesystem_example.id)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_synapse_workspace_example.override_logical_id("example")
        azurerm_synapse_spark_pool_example =
        azurerm.synapse_spark_pool.SynapseSparkPool(self, "example_4",
            auto_pause=SynapseSparkPoolAutoPause(
                delay_in_minutes=15
            ),
            auto_scale=SynapseSparkPoolAutoScale(
                max_node_count=50,
                min_node_count=3
            ),
            cache_size=100,
            library_requirement=SynapseSparkPoolLibraryRequirement(
                content="appnope==0.1.0\nbeautifulsoup4==4.6.3\n",
                filename="requirements.txt"
            ),
            name="example",
            node_size="Small",
            node_size_family="MemoryOptimized",
            spark_config=SynapseSparkPoolSparkConfig(
                content="spark.shuffle.spill                true\n",
                filename="config.txt"
            ),
            synapse_workspace_id=cdktf.Token.as_string(azurerm_synapse_workspace_example.id),
            tags={
                "ENV": "Production"
            }
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        azurerm_synapse_spark_pool_example.override_logical_id("example")
```

## Arguments Reference

The following arguments are supported:

* `name` - (Required) The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.

* `synapse_workspace_id` - (Required) The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.

* `node_size_family` - (Required) The kind of nodes that the Spark Pool provides. Possible values are `MemoryOptimized` and `None`.

* `node_size` - (Required) The level of node in the Spark Pool. Possible values are `Small`, `Medium`, `Large`, `None`, `XLarge`, `XXLarge` and `XXXLarge`.

* `node_count` - (Optional) The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.

* `auto_scale` - (Optional) An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.

* `auto_pause` - (Optional) An `auto_pause` block as defined below.

* `cache_size` - (Optional) The cache size in the Spark Pool.

* `compute_isolation_enabled` - (Optional) Indicates whether compute isolation is enabled or not. Defaults to `false`.

~> **NOTE:** The `compute_isolation_enabled` is only available with the XXXLarge (80 vCPU / 504 GB) node size and only available in the following regions: East US, West US 2, South Central US, US Gov Arizona, US Gov Virginia. See [Isolated Compute](https://docs.microsoft.com/azure/synapse-analytics/spark/apache-spark-pool-configurations#isolated-compute) for more information.

* `dynamic_executor_allocation_enabled` - (Optional) Indicates whether Dynamic Executor Allocation is enabled or not. Defaults to `false`.

* `min_executors` - (Optional) The minimum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.

* `max_executors` - (Optional) The maximum number of executors allocated only when `dynamic_executor_allocation_enabled` set to `true`.
  
* `library_requirement` - (Optional) A `library_requirement` block as defined below.

* `session_level_packages_enabled` - (Optional) Indicates whether session level packages are enabled or not. Defaults to `false`.

* `spark_config` - (Optional) A `spark_config` block as defined below.

* `spark_log_folder` - (Optional) The default folder where Spark logs will be written. Defaults to `/logs`.

* `spark_events_folder` - (Optional) The Spark events folder. Defaults to `/events`.

* `spark_version` - (Optional) The Apache Spark version. Possible values are `2.4` , `3.1` , `3.2` and `3.3`. Defaults to `2.4`.

* `tags` - (Optional) A mapping of tags which should be assigned to the Synapse Spark Pool.

---

An `auto_pause` block supports the following:

* `delay_in_minutes` - (Required) Number of minutes of idle time before the Spark Pool is automatically paused. Must be between `5` and `10080`.

---

An `auto_scale` block supports the following:

* `max_node_count` - (Required) The maximum number of nodes the Spark Pool can support. Must be between `3` and `200`.

* `min_node_count` - (Required) The minimum number of nodes the Spark Pool can support. Must be between `3` and `200`.

---

An `library_requirement` block supports the following:

* `content` - (Required) The content of library requirements.

* `filename` - (Required) The name of the library requirements file.

---

An `spark_config` block supports the following:

* `content` - (Required) The contents of a spark configuration.

* `filename` - (Required) The name of the file where the spark configuration `content` will be stored.

## Attributes Reference

In addition to the Arguments listed above - the following Attributes are exported:

* `id` - The ID of the Synapse Spark Pool.

## Timeouts

The `timeouts` block allows you to specify [timeouts](https://www.terraform.io/language/resources/syntax#operation-timeouts) for certain actions:

* `create` - (Defaults to 30 minutes) Used when creating the Synapse Spark Pool.
* `read` - (Defaults to 5 minutes) Used when retrieving the Synapse Spark Pool.
* `update` - (Defaults to 30 minutes) Used when updating the Synapse Spark Pool.
* `delete` - (Defaults to 30 minutes) Used when deleting the Synapse Spark Pool.

## Import

Synapse Spark Pool can be imported using the `resource id`, e.g.

```shell
terraform import azurerm_synapse_spark_pool.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
```

<!-- cache-key: cdktf-0.17.0-pre.15 input-34a4ca362bab762e262355044fe626a95f2ccd30a1f3762fbb4cad31fdc67558 -->